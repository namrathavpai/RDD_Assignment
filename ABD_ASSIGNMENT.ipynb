{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDD_ASSIGNMENT\").getOrCreate()\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create RDDs in three different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello world', 'Hope you are bot fed with ABD class', 'ello']\n"
     ]
    }
   ],
   "source": [
    "#1. Using parallelize method\n",
    "rdd_par = spark.sparkContext.parallelize([\"Hello world\", \"Hope you are bot fed with ABD class\", \"ello\"])\n",
    "print(rdd_par.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello world', 'Hope you are bot fed with ABD class']\n"
     ]
    }
   ],
   "source": [
    "#2. Using transformation\n",
    "rdd_trans = rdd_par.filter(lambda word : word.startswith('H'))\n",
    "print(rdd_trans.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 2 3 4 5 ', '3 4 5', '7 8 56 78 90 23 ', '4 5 67 34 5']\n"
     ]
    }
   ],
   "source": [
    "#3. Using data source\n",
    "rdd_ds = spark.sparkContext.textFile('number_text_input.txt')\n",
    "print(rdd_ds.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read a text file and count number of words in the file using RDD operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "world_count_file = spark.sparkContext.textFile('world_count_input.txt')\n",
    "word_count = world_count_file.filter(lambda x: x.split(' '))\n",
    "words = word_count.flatMap(lambda x: x.split(' '))\n",
    "print(words.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a program to find the word frequency in a given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi this is namratha', 'I study in BDA', 'big data and data analytics']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hi', 1),\n",
       " ('this', 1),\n",
       " ('is', 1),\n",
       " ('in', 1),\n",
       " ('BDA', 1),\n",
       " ('analytics', 1),\n",
       " ('namratha', 1),\n",
       " ('I', 1),\n",
       " ('study', 1),\n",
       " ('big', 1),\n",
       " ('data', 2),\n",
       " ('and', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_frequency = world_count_file.flatMap(lambda word: word.split(' '))\n",
    "print(world_count_file.collect())\n",
    "frequency_of_words = world_frequency.map(lambda word: (word, 1))\n",
    "frequency_of_words.reduceByKey(lambda a,b : a + b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Write a program to convert all words in a file to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HI', 'THIS', 'IS', 'NAMRATHA', 'I', 'STUDY', 'IN', 'BDA', 'BIG', 'DATA', 'AND', 'DATA', 'ANALYTICS']\n"
     ]
    }
   ],
   "source": [
    "world_to_upper = world_count_file.flatMap(lambda word: word.split(' '))\n",
    "word_upper=world_to_upper.map(lambda word:word.upper())\n",
    "print(word_upper.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write a program to convert all words in a file to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'this', 'is', 'namratha', 'i', 'study', 'in', 'bda', 'big', 'data', 'and', 'data', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "world_to_lower = world_count_file.flatMap(lambda word: word.split(' '))\n",
    "word_lower=world_to_lower.map(lambda word:word.lower())\n",
    "print(word_lower.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a program to capitalize first leter of each words in file (use string capitalize() method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'This', 'Is', 'Namratha', 'I', 'Study', 'In', 'Bda', 'Big', 'Data', 'And', 'Data', 'Analytics']\n"
     ]
    }
   ],
   "source": [
    "world_to_cap = world_count_file.flatMap(lambda word: word.split(' '))\n",
    "word_cap=world_to_.map(lambda word:word.capitalize())\n",
    "print(word_cap.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Find the longest length of word from given set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'this', 'is', 'namratha', 'I', 'study', 'in', 'BDA', 'big', 'data', 'and', 'data', 'analytics']\n",
      "analytics\n"
     ]
    }
   ],
   "source": [
    "world_to_len = world_count_file.flatMap(lambda word: word.split(' '))\n",
    "print(world_to_len.collect())\n",
    "word = world_to_len.map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0]))\n",
    "longest_word = ''\n",
    "for word in word.collect():\n",
    "    if len(word) > len(longest_word):\n",
    "        longest_word = word\n",
    "print(longest_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Map the Registration numbers to corresponding branch. 6000 series BDA, 9000 series HDA, 1000 series ML, 2000 series VLSI, 3000 series ES, 4000 series MSc, 5000 series CC. Given registration number, generate a key-value pair of Registration Number and Corresponding Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|code|\n",
      "+----+\n",
      "|6000|\n",
      "|9000|\n",
      "|1000|\n",
      "|2000|\n",
      "|3000|\n",
      "|4000|\n",
      "|5000|\n",
      "+----+\n",
      "\n",
      "+----+------+\n",
      "|code|branch|\n",
      "+----+------+\n",
      "|6000|   BDA|\n",
      "|9000|   HDA|\n",
      "|1000|    ML|\n",
      "|2000|  VLSI|\n",
      "|3000|    ES|\n",
      "|4000|   MSC|\n",
      "|5000|    CC|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns=['curremcy','value']\n",
    "inputdata=[(6000, ),(9000,),(1000,),(2000,),(3000,),(4000,),(5000,)]\n",
    "\n",
    "def determine_branch(code):\n",
    "    if code >=6000 and code <7000:\n",
    "        return 'BDA'\n",
    "    elif code >=9000 and code<10000:\n",
    "        return 'HDA'\n",
    "    elif code>=1000 and code<2000:\n",
    "        return 'ML'\n",
    "    elif code>=2000 and code<3000:\n",
    "        return 'VLSI'\n",
    "    elif code>=3000 and code<4000:\n",
    "        return 'ES'\n",
    "    elif code>=4000 and code<5000:\n",
    "        return 'MSC'\n",
    "    elif code>=5000 and code<6000:\n",
    "        return 'CC'\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(inputdata)\n",
    "rdd_df=rdd.toDF()\n",
    "df=rdd_df.withColumnRenamed('_1','code')\n",
    "df.show()\n",
    "branch_udf=udf(determine_branch, StringType())\n",
    "df.withColumn('branch',branch_udf(col('code'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Text file contain numbers. Numbers are separated by one white space. There is no order to store the numbers. One line may contain one or more numbers. Find the maximum, minimum, sum and mean of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 2 3 4 5', '3 4 5', '7 8 56 78 90 23', '4 5 67 34 5']\n",
      "90\n",
      "1\n",
      "404\n",
      "21.263157894736846\n"
     ]
    }
   ],
   "source": [
    "rdd_ds = spark.sparkContext.textFile('number_text_input.txt')\n",
    "rdd_ds_num = rdd_ds.flatMap(lambda n: n.split(' '))\n",
    "rdd_ds_num = rdd_ds_num.map(lambda n: int(n))\n",
    "print(rdd_ds.collect())\n",
    "print(rdd_ds_num.max())\n",
    "print(rdd_ds_num.min())\n",
    "print(rdd_ds_num.sum())\n",
    "print(rdd_ds_num.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name, dob, Phone, email and state name. Another file contains mapping of state names to state code like Karnataka is codes as KA, TamilNadu as TN, Kerala KL etc. Compress the citizen.txt file by changing full state name to state code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+-------------+----------+\n",
      "|    Name|       dob|     Phone|        email|state code|\n",
      "+--------+----------+----------+-------------+----------+\n",
      "|namratha|24-05-1996|9916240396|nam@gmail.com|        KA|\n",
      "|namratha|24-05-1996|9916240396|nam@gmail.com|        KA|\n",
      "|   pooja|06-01-1966|7411419476|poo@gmail.com|        KA|\n",
      "|   pooja|06-01-1966|7411419476|poo@gmail.com|        KA|\n",
      "|   kavya|13-06-1998|9448792883|kav@gmail.com|        MH|\n",
      "+--------+----------+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_ds_citizen = spark.sparkContext.textFile('citizen.txt')\n",
    "rdd_ds_statecode = spark.sparkContext.textFile('states.txt')\n",
    "rdd_ds_citizen = rdd_ds_citizen.map(lambda citizen: citizen.split(', '))\n",
    "rdd_ds_statecode = rdd_ds_statecode.map(lambda state: state.split(', '))\n",
    "citizen = spark.createDataFrame(rdd_ds_citizen, ['Name', 'dob', 'Phone', 'email', 'state name'])\n",
    "statecodes = spark.createDataFrame(rdd_ds_statecode, ['state name', 'state code'])\n",
    "citizen.collect()\n",
    "statecodes.collect()\n",
    "citizen.join(statecodes, on='state name', how='left').drop('state name').show()\n",
    "citizen.write.csv('citizen_compressed.csv')\n",
    "citizen.rdd.map(lambda x: x[0] + \",\" + str(x)).repartition(1).saveAsTextFile('Text/citizen.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
